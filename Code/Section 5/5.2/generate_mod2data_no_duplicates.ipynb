{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation for Section 5.2 Experiments\n",
    "\n",
    "This notebook generates the datasets used for training the experiments in **Section 5.2** for predicting $a_p \\bmod 2$ given the sequence $(a_q \\bmod 2)_{q \\ne p,\\, q < 100}$ and removing duplicate rows.\n",
    "\n",
    "### Data Source\n",
    "\n",
    "The required data is loaded from the file [`ECQ7apmod2_1e3_unique.txt`](https://zenodo.org/records/15660733), which contains the sequence $(a_q \\bmod 2)_{q < 100}$ for elliptic curves $E$ with conductor $N(E) < 10^7$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and basic functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def encode_integer(val, base=1000, digit_sep=\" \"):\n",
    "    if val == 0:\n",
    "        return '+ 0'\n",
    "    sgn = '+' if val >= 0 else '-'\n",
    "    val = abs(val)\n",
    "    r = []\n",
    "    while val > 0:\n",
    "        r.append(str(val % base))\n",
    "        val = val//base\n",
    "    r.append(sgn)\n",
    "    r.reverse()\n",
    "    return digit_sep.join(r)\n",
    "    \n",
    "def encode_pinteger(val, p):\n",
    "    return '+ '+str(p)+str(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06e6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the list of primes for the column names\n",
    "ps = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n",
    "\n",
    "\n",
    "# For each prime p, generate the dataset to predict a_p mod 2 \n",
    "for p in ps:\n",
    "    print(f\"Prime is {p}\")\n",
    "    column_names =  ['a_' + str(q) for q in ps] + ['conductor', 'rank']\n",
    "\n",
    "    # Prepare to read the data\n",
    "    data = []\n",
    "\n",
    "    # Read from the file\n",
    "    with open('ECQ7apmod2_1e3_unique.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(':')\n",
    "            # Extract data\n",
    "            if parts[0]=='':\n",
    "                a=None\n",
    "            else:\n",
    "                a = int(parts[0])\n",
    "            if parts[4]=='':\n",
    "                d=None\n",
    "            else:\n",
    "                d = int(parts[4])\n",
    "            if a % p !=0:\n",
    "                L3 = list(map(int, parts[7].strip('[]').split(',')))  \n",
    "                # Append to data list\n",
    "                data.append(L3+[a,d])\n",
    "\n",
    "    # Create main dataframe df\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    df.dropna(inplace=True)\n",
    "    df['rank']=df['rank'].apply(lambda x: int(x))\n",
    "    df['cond10'] = np.floor(np.log10(df['conductor']))\n",
    "    \n",
    "    \n",
    "    for q in ps:\n",
    "        if q !=p:\n",
    "            df['a_'+str(q)] = df['a_'+str(q)].apply(lambda x: encode_integer(x)) \n",
    "        if q ==p:\n",
    "            df['a_'+str(q)] = df['a_'+str(q)].apply(lambda x: x)\n",
    "    df[\"data_type\"]=\"V\"+str(len(ps)-1)\n",
    "\n",
    "    #Balance the dataframe\n",
    "    dfc0 = df[df['a_'+str(p)] == 0].sample(df['a_'+str(p)].value_counts().min(), random_state=42)\n",
    "    dfc1 = df[df['a_'+str(p)] == 1]\n",
    "\n",
    "    dfc = pd.concat([dfc0, dfc1], axis=0)\n",
    "    dfc = dfc.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Create the test set\n",
    "    # Sample 5000 entries for each specified log value of conductor\n",
    "\n",
    "    test_set_cond4_aq0 = dfc[(dfc['cond10'] == 4.0)& (dfc['a_'+str(p)] == 0)].sample(n=2500, random_state=42)\n",
    "    test_set_cond4_aq1 = dfc[(dfc['cond10'] == 4.0)& (dfc['a_'+str(p)] == 1)].sample(n=2500, random_state=42)\n",
    "\n",
    "    test_set_cond5_aq0 = dfc[(dfc['cond10'] == 5.0)& (dfc['a_'+str(p)] == 0)].sample(n=2500, random_state=42)\n",
    "    test_set_cond5_aq1 = dfc[(dfc['cond10'] == 5.0)& (dfc['a_'+str(p)] == 1)].sample(n=2500, random_state=42)\n",
    "\n",
    "    test_set_cond6_aq0 = dfc[(dfc['cond10'] == 6.0)& (dfc['a_'+str(p)] == 0)].sample(n=2500, random_state=42)\n",
    "    test_set_cond6_aq1 = dfc[(dfc['cond10'] == 6.0)& (dfc['a_'+str(p)] == 1)].sample(n=2500, random_state=42)\n",
    "    # Concatenate to form the test set\n",
    "    df_test_cond = pd.concat([test_set_cond4_aq0, test_set_cond4_aq1,test_set_cond5_aq0, test_set_cond5_aq1, test_set_cond6_aq0, test_set_cond6_aq1])\n",
    "\n",
    "\n",
    "\n",
    "    # Create the training set by excluding the indices used in the test set\n",
    "    df_train = dfc.drop(df_test_cond.index)\n",
    "    \n",
    "    # Create the data files\n",
    "    df_train1=df_train[['data_type']+['a_'+str(q) for q in ps if p !=q]+['a_'+str(p)]]\n",
    "    df_test_cond1=df_test_cond[['data_type']+['a_'+str(q) for q in ps if q != p]+['a_'+str(p)]]\n",
    "    \n",
    "    print(df_train1)\n",
    "    # Training dataset\n",
    "    dftoint_train = pd.DataFrame()\n",
    "    dftoint_train['input'] =  df_train1.iloc[:, :-1].agg(' '.join, axis=1)\n",
    "    dftoint_train['output'] = df_train1['a_'+str(p)]\n",
    "    dftoint_train.to_csv(\"apmod2_to_a\"+str(p)+\"mod2_distinct_ecq7_train_check.txt\", sep='\\t', index=False, header=False)\n",
    "    \n",
    "\n",
    "    # Test dataset\n",
    "    dftoint_test = pd.DataFrame()\n",
    "    dftoint_test['input'] =  df_test_cond1.iloc[:, :-1].agg(' '.join, axis=1)\n",
    "    dftoint_test['output'] = df_test_cond1['a_'+str(p)].apply(lambda x : x)\n",
    "    dftoint_test.to_csv(\"apmod2_to_a\"+str(p)+\"mod2_distinct_ecq7_test_check.txt\", sep='\\t', index=False, header=False)\n",
    "    \n",
    "\n",
    "    # Delete intermediate dataframes \n",
    "    del df\n",
    "    del dfc\n",
    "    del df_train1\n",
    "    del df_train\n",
    "    del dftoint_test\n",
    "    del dftoint_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
